---
title: "Why Use Great Tables with LLMs?"
author: Rich Iannone
date: 2025-11-13
jupyter: python3
format:
    html:
        embed-resources: true
html-table-processing: none
---

## The Problem: LLMs Love Tables, But Notebooks Need Better

If you've used Claude, ChatGPT, or other LLM interfaces lately, you've noticed something: they love presenting information in tables. And for good reason. Tables are incredibly effective at organizing structured information.

But here's the disconnect: when you're analyzing data in a Python notebook, you need tables that live in your code, not just in a chat interface. You want the same control over tables that you have with plotnine or seaborn for visualizations. You want reproducible, extensible, and beautiful tables that work with your data pipelines.

This is where Great Tables shines. Great Tables is a Python library designed for quickly generating sophisticated tables in your notebook, whether you're writing the code yourself or working with an LLM assistant. More importantly, it gives you something Markdown tables can't: a programmatic recipe that adapts to your data and extends with your needs.

Let's explore why Great Tables is the ideal choice when working with LLMs to analyze and present data.

## The Progression: Text -> Markdown -> Great Tables

To understand the value of Great Tables, let's see how the same information looks when presented in three different ways. We'll use a realistic scenario: analyzing GitHub repository metrics.

### Stage 1: Paragraph Text

Here's how an LLM might initially describe repository data:

```
The great-tables repository has 1,834 stars and 67 forks, with 23 open issues and 234 closed issues. It has merged 456 pull requests and the last commit was 2 days ago. The polars repository is much larger with 23,456 stars and 1,234 forks, currently has 345 open issues with 5,678 closed, has merged 8,934 pull requests, and was updated 1 day ago. The pandas repository is the largest with 38,934 stars...
```

The problem with this chunk of text is that it is exhausting to read. Finding patterns or comparing values requires some mental gymnastics here. You can't quickly spot which repos are most active or which need attention.

### Stage 2: Markdown Table

The natural next step is to structure this information in a table format. This is exactly what LLMs often do when you ask them to present data more clearly. Let's improve this with a Markdown table:

```markdown
| Repository | Language | Stars | Forks | Open Issues | Closed Issues | PRs Merged | Days Since Commit |
|------------|----------|-------|-------|-------------|---------------|------------|-------------------|
| great-tables | Python | 1834 | 67 | 23 | 234 | 456 | 2 |
| polars | Rust | 23456 | 1234 | 345 | 5678 | 8934 | 1 |
| pandas | Python | 38934 | 15678 | 2345 | 23456 | 12345 | 3 |
| duckdb | C++ | 15234 | 890 | 123 | 3456 | 5678 | 1 |
| quarto-cli | TypeScript | 8234 | 456 | 89 | 1234 | 2345 | 5 |
| plotly.py | Python | 13456 | 2345 | 234 | 4567 | 6789 | 4 |
```

Much better! The table structure makes comparisons easier. But we're still facing limitations:

- numbers aren't formatted for readability (1834 vs 1,834)
- no visual cues for what's important
- can't highlight concerning patterns (old commits, high open issues)
- static, so if data changes, you need to regenerate the entire table
- no room for additional context (titles, footnotes, sources)

### Stage 3: Great Tables

Now let's see the same data as a Great Tables table. Here's an example prompt you might use:

```
Create a Great Tables table from `data/github_repos.csv` showing repository metrics. Format the numbers with thousands separators, add a color gradient on the stars column (blue palette), and color-code the days since last commit (green for recent, red for old). Include a title, subtitle, and group the issues columns under a spanner labeled 'Issues'.
```

Here's what the LLM would generate:

```{python}
from great_tables import GT, md, html
import polars as pl

# Load the data
repos = pl.read_csv("data/github_repos.csv")

# Create a Great Tables table
(
    GT(repos)
    .tab_header(
        title="GitHub Repository Metrics",
        subtitle="Popular data science and visualization libraries"
    )
    .fmt_number(
        columns=["stars", "forks", "issues_open", "issues_closed", "prs_merged"],
        decimals=0,
        use_seps=True
    )
    .data_color(
        columns="stars",
        palette="Blues",
        domain=[0, 40000]
    )
    .data_color(
        columns="last_commit_days",
        palette=["#90EE90", "#FFD700", "#FF6B6B"],
        domain=[0, 7]
    )
    .cols_label(
        repo="Repository",
        language="Language",
        stars="Stars",
        forks="Forks",
        issues_open="Open",
        issues_closed="Closed",
        prs_merged="Merged",
        last_commit_days="Days Since Commit"
    )
    .tab_spanner(
        label="Issues",
        columns=["issues_open", "issues_closed"]
    )
    .tab_spanner(
        label="Activity",
        columns=["prs_merged", "last_commit_days"]
    )
    .tab_source_note(
        source_note="Data collected from GitHub API on November 5, 2025"
    )
)
```

This is the sweet spot and we get:

- visual hierarchy: color gradients instantly show popularity and recency
- proper formatting: numbers with thousands separators
- structure: spanners group related columns
- context: title, subtitle, and source notes
- reproducibility: change the data, run the code, get an updated table!
- extensibility: easy to add styling, formatting, or new columns

Most importantly: **you have the code**. This isn't just a pretty table. It's a recipe you can modify, reuse, and adapt.
Now that we've seen the basic progression from text to Markdown to Great Tables, let's dive into specific scenarios where this combination of LLM assistance and Great Tables functionality creates something truly powerful. Each scenario demonstrates a different strength of Great Tables that's particularly well-suited to LLM-generated content and human analytical needs.

### 1. Trend Visualization with Nanoplots

One of Great Tables' superpowers is `.fmt_nanoplot()`. This method provides the ability to embed sparklines directly in table cells. This is perfect for LLM-generated trend data.

Let's suppose you're asking an LLM to analyze quarterly sales trends. The LLM can easily generate lists of numbers, but humans process visual trends far better than number sequences.

Here's an example prompt:

```
I have sales data by region for Q4 2025. Create a Great Tables table that shows: region, current quarter sales (formatted with commas), a quarterly trend as a sparkline using fmt_nanoplot, and year-over-year growth as a percentage. Use a red-to-green color gradient on the growth column to highlight positive vs negative growth.
```

```{python}
# Create sample data with trends
sales_data = pl.DataFrame({
    "region": ["North", "South", "East", "West", "Central"],
    "current_quarter": [245000, 189000, 312000, 276000, 198000],
    "trend": [
        "195,210,228,245",
        "212,198,185,189",
        "278,289,301,312",
        "245,258,271,276",
        "178,185,192,198"
    ],
    "yoy_growth": [0.127, -0.043, 0.185, 0.098, 0.056]
})

(
    GT(sales_data)
    .tab_header(
        title="Regional Sales Performance",
        subtitle="Q4 2025 with quarterly trend"
    )
    .fmt_number(
        columns="current_quarter",
        decimals=0,
        use_seps=True
    )
    .fmt_percent(
        columns="yoy_growth",
        decimals=1
    )
    .fmt_nanoplot(
        columns="trend",
        plot_type="line",
        autoscale=True
    )
    .data_color(
        columns="yoy_growth",
        palette=["#FF6B6B", "#FFFFFF", "#90EE90"],
        domain=[-0.1, 0.2]
    )
    .cols_label(
        region="Region",
        current_quarter="Q4 Sales",
        trend="Quarterly Trend",
        yoy_growth="YoY Growth"
    )
)
```

The alternative in Markdown might be numbers presented like this:

```
Q1: 195, Q2: 210, Q3: 228, Q4: 245
```

This is functional but nowhere near as immediately comprehensible.

### 2. Conditional Formatting for Data Quality Checks

LLMs are great at generating data summaries, but you need to quickly spot anomalies or issues. Great Tables makes this trivial.

Here's an example prompt:

```
Load `data/api_latency.csv` and create a Great Tables table showing API endpoint performance. Calculate the error rate as a percentage. Use color gradients to highlight slow endpoints (p95_ms) and high error rates. Use green for good, yellow for concerning, red for critical. Format numbers appropriately and add a source note explaining the color coding.
```

```{python}
# API latency data
api_data = pl.read_csv("data/api_latency.csv")

# Calculate error rate
api_data = api_data.with_columns(
    (pl.col("errors") / pl.col("requests") * 100).alias("error_rate")
)

(
    GT(api_data)
    .tab_header(
        title="API Endpoint Performance",
        subtitle="Production metrics from last 24 hours"
    )
    .fmt_number(
        columns=["avg_ms", "p95_ms", "p99_ms"],
        decimals=0
    )
    .fmt_number(
        columns="requests",
        decimals=0,
        use_seps=True
    )
    .fmt_percent(
        columns="error_rate",
        decimals=2,
        scale_values=False
    )
    .data_color(
        columns="p95_ms",
        palette=["#90EE90", "#FFD700", "#FF6B6B"],
        domain=[0, 1500]
    )
    .data_color(
        columns="error_rate",
        palette=["#90EE90", "#FFD700", "#FF6B6B"],
        domain=[0, 1.0]
    )
    .cols_label(
        endpoint="Endpoint",
        method="Method",
        avg_ms="Avg (ms)",
        p95_ms="P95 (ms)",
        p99_ms="P99 (ms)",
        requests="Requests",
        errors="Errors",
        error_rate="Error Rate"
    )
    .tab_source_note(
        source_note="Red indicates endpoints exceeding SLA thresholds"
    )
)
```

This approach is powerful for several reasons. Color gradients instantly highlight problem areas like slow endpoints and high error rates. You can immediately see that `/api/upload` needs attention without scanning every row. The LLM generates the data and basic structure, while Great Tables adds the intelligence layer through conditional formatting. Proper formatting with thousands separators and percentages makes numbers instantly scannable.

In contrast, Markdown tables fall short here. Static text simply can't convey urgency or priority. You'd have to manually scan each row, comparing numbers mentally, and hope you catch the problematic patterns.
Great Tables supports structural elements that make complex tables comprehensible.

Here's an example prompt:

```
Create a Great Tables table from `data/tech_salaries.csv`. Group rows by role, format all compensation columns as currency, add a color gradient to the total compensation column (green shades). Use a spanner to group the compensation columns, and add source notes explaining the data source and abbreviations.
```

```{python}
# Tech salaries data
salaries = pl.read_csv("data/tech_salaries.csv")

(
    GT(salaries, groupname_col="role")
    .tab_header(
        title="Tech Compensation by Role and Level",
        subtitle="Major US tech companies, 2025"
    )
    .fmt_currency(
        columns=["base_salary", "bonus", "equity", "total_comp"],
        currency="USD",
        decimals=0
    )
    .data_color(
        columns="total_comp",
        palette="Greens",
        domain=[200000, 900000]
    )
    .cols_label(
        level="Level",
        base_salary="Base",
        bonus="Bonus",
        equity="Equity",
        total_comp="Total",
        yoe="YoE",
        location="Location"
    )
    .tab_spanner(
        label="Compensation Components",
        columns=["base_salary", "bonus", "equity", "total_comp"]
    )
    .tab_source_note(
        source_note="YoE = Years of Experience"
    )
    .tab_source_note(
        source_note="Data aggregated from levels.fyi and Blind"
    )
)
```

This example showcases several structural features that make Great Tables particularly powerful. Row groups automatically organize entries by role, creating clear visual separation. Spanners group related columns under descriptive headers, making the compensation structure immediately clear. The title and subtitle provide essential context at a glance. Source notes add credibility and explain abbreviations like "YoE." And currency formatting ensures proper symbols and separators throughout.
