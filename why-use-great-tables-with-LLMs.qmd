---
title: "Why Use Great Tables with LLMs?"
author: Rich Iannone
date: 2025-11-15
jupyter: python3
format:
    html:
        embed-resources: true
html-table-processing: none
---

## The Problem: LLMs Love Tables, But Notebooks Need Better

If you've used Claude, ChatGPT, or other LLM interfaces lately, you've noticed something: they love presenting information in tables. And for good reason. Tables are incredibly effective at organizing structured information.

But here's the disconnect: when you're analyzing data in a Python notebook, you need tables that live in your code, not just in a chat interface. You want the same control over tables that you have with [plotnine](https://plotnine.org) or [seaborn](https://seaborn.pydata.org) for visualizations. You want reproducible, extensible, and beautiful tables that work with your data pipelines.

This is where [Great Tables](https://posit-dev.github.io/great-tables/) shines. Great Tables is a Python library designed for quickly generating sophisticated tables in your notebook, whether you're writing the code yourself or working with an LLM assistant. More importantly, it gives you something Markdown tables can't: a programmatic recipe that adapts to your data and extends with your needs.

Let's explore why Great Tables is the ideal choice when working with LLMs to analyze and present data.

## But Do LLMs Actually Work with Great Tables?

Before diving into the *why*, let's address the *does it work?* question with actual data. We systematically tested Claude Sonnet 4.5 on three progressively complex scenarios: (1) a simple table with basic formatting requirements, (2) a salary table with row grouping and currency formatting, and (3) a repository stats table with nanoplots and multi-level styling.

Here's what we found:

![](GT-LLM-quality-claude-sonnet-4-5.png)

The results are compelling. Even without API documentation, the LLM generated high-quality code for simpler tables. With API documentation included in the prompt, it achieved excellent results across all complexity levels—including the most challenging example with nanoplots and complex styling.

This success isn't accidental. We've deliberately designed Great Tables to be LLM-friendly: composable methods that chain naturally, consistent naming patterns, and declarative syntax that maps intuitively to how you'd describe a table in plain language. When an LLM sees a request to "format numbers with thousands separators and replace missing values with a hyphen," it can readily translate that to `.fmt_number(use_seps=True)` and `.sub_missing(missing_text="-")`.

What this means for your workflow: LLMs can reliably generate Great Tables code, especially when given appropriate context. If you're using GitHub Copilot, Claude, or ChatGPT, providing API documentation or examples dramatically improves first-attempt success. And even when the LLM gets something wrong, you can simply point out the error and get a corrected version immediately. The combination of thoughtful API design and powerful language models creates a remarkably effective partnership for building sophisticated tables. 

## The Progression: Text -> Markdown -> Great Tables

To understand the value of Great Tables, let's see how the same information looks when presented in three different ways. We'll use a realistic scenario: analyzing GitHub repository metrics.

### Stage 1: Paragraph Text

Here's how an LLM might initially describe repository data:

```
The great-tables repository has 1,834 stars and 67 forks, with 23 open issues and
234 closed issues. It has merged 456 pull requests and the last commit was 2 days
ago. The polars repository is much larger with 23,456 stars and 1,234 forks,
currently has 345 open issues with 5,678 closed, has merged 8,934 pull requests,
and was updated 1 day ago. The pandas repository is the largest with 38,934 stars...
```

The problem with this chunk of text is that it is exhausting to read. Finding patterns or comparing values requires some mental gymnastics here. You can't quickly spot which repos are most active or which need attention.

### Stage 2: Markdown Table

The natural next step is to structure this information in a table format. This is exactly what LLMs often do when you ask them to present data more clearly. Let's improve this with a Markdown table:

| Repository | Language | Stars | Forks | Open Issues | Closed Issues | PRs Merged | Days Since Commit |
|------------|----------|-------|-------|-------------|---------------|------------|-------------------|
| great-tables | Python | 1834 | 67 | 23 | 234 | 456 | 2 |
| polars | Rust | 23456 | 1234 | 345 | 5678 | 8934 | 1 |
| pandas | Python | 38934 | 15678 | 2345 | 23456 | 12345 | 3 |
| duckdb | C++ | 15234 | 890 | 123 | 3456 | 5678 | 1 |
| quarto-cli | TypeScript | 8234 | 456 | 89 | 1234 | 2345 | 5 |
| plotly.py | Python | 13456 | 2345 | 234 | 4567 | 6789 | 4 |

Much better! The table structure makes comparisons easier. But we're still facing limitations:

- numbers aren't formatted for readability (1834 vs 1,834)
- no visual cues for what's important
- can't highlight concerning patterns (old commits, high open issues)
- static, so if data changes, you need to regenerate the entire table
- no room for additional context (titles, footnotes, sources)

### Stage 3: Great Tables

Now let's see the same data as a Great Tables table. Here's an example prompt you might use:

```
Create a Great Tables table from `data/github_repos.csv` showing repository
metrics. Format the numbers with thousands separators, add a color gradient on
the stars column (blue palette), and color-code the days since last commit
(green for recent, red for old). Include a title, subtitle, and group the
issues columns under a spanner labeled 'Issues'.
```

Here's what the LLM would generate:

```{python}
from great_tables import GT, md, html
import polars as pl

# Load the data
repos = pl.read_csv("data/github_repos.csv")

# Create a Great Tables table
(
    GT(repos)
    .tab_header(
        title="GitHub Repository Metrics",
        subtitle="Popular data science and visualization libraries"
    )
    .fmt_number(
        columns=["stars", "forks", "issues_open", "issues_closed", "prs_merged"],
        decimals=0,
        use_seps=True
    )
    .data_color(
        columns="stars",
        palette="Blues",
        domain=[0, 40000]
    )
    .data_color(
        columns="last_commit_days",
        palette=["#90EE90", "#FFD700", "#FF6B6B"],
        domain=[0, 7]
    )
    .cols_label(
        repo="Repository",
        language="Language",
        stars="Stars",
        forks="Forks",
        issues_open="Open",
        issues_closed="Closed",
        prs_merged="Merged",
        last_commit_days="Days Since Commit"
    )
    .tab_spanner(
        label="Issues",
        columns=["issues_open", "issues_closed"]
    )
    .tab_spanner(
        label="Activity",
        columns=["prs_merged", "last_commit_days"]
    )
    .tab_source_note(
        source_note="Data collected from GitHub API on November 5, 2025"
    )
)
```

This is the sweet spot and we get:

- visual hierarchy: color gradients instantly show popularity and recency
- proper formatting: numbers with thousands separators
- structure: spanners group related columns
- context: title, subtitle, and source notes
- reproducibility: change the data, run the code, get an updated table!
- extensibility: easy to add styling, formatting, or new columns

The most important thing is that *you have the code*. This isn't just a pretty table. It's a recipe you can modify, reuse, and adapt.

## Why Great Tables Works Great with LLMs: Five Key Scenarios

Now that we've seen the basic progression from text to Markdown to Great Tables (and know from our testing that LLMs can reliably generate this code when given proper context), let's dive into specific scenarios where this combination creates something truly powerful. 

These scenarios are drawn from real examples we tested with frontier models. The first scenario (API latency dashboard with basic formatting and conditional styling) had a 100% success rate even without documentation. As complexity increased, so did the importance of providing API context. Each scenario demonstrates a different strength of Great Tables that's particularly well-suited to LLM-generated content and human analytical needs.

### 1. Trend Visualization with Nanoplots

One of Great Tables' superpowers is `.fmt_nanoplot()`. This method provides the ability to embed sparklines directly in table cells. This is perfect for LLM-generated trend data.

Let's suppose you're asking an LLM to analyze quarterly sales trends. The LLM can easily generate lists of numbers, but humans process visual trends far better than number sequences.

Here's an example prompt:

```
Load `data/sales_trends.csv`, which is sales data by region for Q4 2025.
Create a Great Tables table that shows: region, current quarter sales
(formatted with commas), a quarterly trend as a sparkline using
fmt_nanoplot, and year-over-year growth as a percentage. Use a
red-to-green color gradient on the growth column to highlight positive
vs negative growth.
```

Here's what the LLM would generate:

```{python}
# Load sales data with trends
sales_data = pl.read_csv("data/sales_trends.csv")

(
    GT(sales_data)
    .tab_header(
        title="Regional Sales Performance",
        subtitle="Q4 2025 with quarterly trend"
    )
    .fmt_number(
        columns="current_quarter",
        decimals=0,
        use_seps=True
    )
    .fmt_percent(
        columns="yoy_growth",
        decimals=1
    )
    .fmt_nanoplot(
        columns="trend",
        plot_type="line",
        autoscale=True
    )
    .data_color(
        columns="yoy_growth",
        palette=["#FF6B6B", "#FFFFFF", "#90EE90"],
        domain=[-0.1, 0.2]
    )
    .cols_label(
        region="Region",
        current_quarter="Q4 Sales",
        trend="Quarterly Trend",
        yoy_growth="YoY Growth"
    )
)
```

Why does this matter? There are a few good reasons: 

- LLMs can easily generate comma-separated trend values (e.g., "195,210,228,245")
- Great Tables transforms these strings into visual sparklines
- you get instant visual verification of trends. Is the South region really declining? Yes, the sparkline shows it clearly
- this bridges the gap between LLM text generation and human visual processing

The alternative in Markdown might be numbers presented like this:

```
Q1: 195, Q2: 210, Q3: 228, Q4: 245
```

This is functional but nowhere near as immediately comprehensible.

### 2. Conditional Formatting for Data Quality Checks

LLMs are great at generating data summaries, but you need to quickly spot anomalies or issues. Great Tables makes this sort of thing pretty trivial.

Here's an example prompt:

```
Load `data/api_latency.csv` and create a Great Tables table showing API
endpoint performance. Calculate the error rate as a percentage. Use color
gradients to highlight slow endpoints (p95_ms) and high error rates. Use
green for good, yellow for concerning, red for critical. Format numbers
appropriately and add a source note explaining the color coding.
```

The LLM returns this:

```{python}
# API latency data
api_data = pl.read_csv("data/api_latency.csv")

# Calculate error rate
api_data = api_data.with_columns(
    (pl.col("errors") / pl.col("requests") * 100).alias("error_rate")
)

(
    GT(api_data)
    .tab_header(
        title="API Endpoint Performance",
        subtitle="Production metrics from last 24 hours"
    )
    .fmt_number(
        columns=["avg_ms", "p95_ms", "p99_ms"],
        decimals=0
    )
    .fmt_number(
        columns="requests",
        decimals=0,
        use_seps=True
    )
    .fmt_percent(
        columns="error_rate",
        decimals=2,
        scale_values=False
    )
    .data_color(
        columns="p95_ms",
        palette=["#90EE90", "#FFD700", "#FF6B6B"],
        domain=[0, 1500]
    )
    .data_color(
        columns="error_rate",
        palette=["#90EE90", "#FFD700", "#FF6B6B"],
        domain=[0, 1.0]
    )
    .cols_label(
        endpoint="Endpoint",
        method="Method",
        avg_ms="Avg (ms)",
        p95_ms="P95 (ms)",
        p99_ms="P99 (ms)",
        requests="Requests",
        errors="Errors",
        error_rate="Error Rate"
    )
    .tab_source_note(
        source_note="Red indicates endpoints exceeding SLA thresholds"
    )
)
```

This approach is powerful for several reasons. Color gradients instantly highlight problem areas like slow endpoints and high error rates. You can immediately see that `/api/upload` needs attention without scanning every row. The LLM generates the data and basic structure, while Great Tables adds the intelligence layer through conditional formatting. Proper formatting with thousands separators and percentages makes numbers instantly scannable.

In contrast, Markdown tables fall short here. Static text simply can't convey urgency or priority. You'd have to manually scan each row, comparing numbers mentally, and hope you catch the problematic patterns.

### 3. Rich Structure That Markdown Can't Provide

When tables grow beyond a handful of columns, organization becomes critical. Great Tables supports structural elements like row groups, column spanners, titles, subtitles, and footnotes that transform raw data into a well-organized, self-documenting presentation. These features make complex tables comprehensible at a glance.

Here's an example prompt:

```
Create a Great Tables table from `data/tech_salaries.csv`. Group rows by role,
format all compensation columns as currency, add a color gradient to the total
compensation column (green shades). Use a spanner to group the compensation
columns, and add source notes explaining the data source and abbreviations.
```

This is the code output from the LLM:

```{python}
# Tech salaries data
salaries = pl.read_csv("data/tech_salaries.csv")

(
    GT(salaries, groupname_col="role")
    .tab_header(
        title="Tech Compensation by Role and Level",
        subtitle="Major US tech companies, 2025"
    )
    .fmt_currency(
        columns=["base_salary", "bonus", "equity", "total_comp"],
        currency="USD",
        decimals=0
    )
    .data_color(
        columns="total_comp",
        palette="Greens",
        domain=[200000, 900000]
    )
    .cols_label(
        level="Level",
        base_salary="Base",
        bonus="Bonus",
        equity="Equity",
        total_comp="Total",
        yoe="YoE",
        location="Location"
    )
    .tab_spanner(
        label="Compensation Components",
        columns=["base_salary", "bonus", "equity", "total_comp"]
    )
    .tab_source_note(
        source_note="YoE = Years of Experience"
    )
    .tab_source_note(
        source_note="Data aggregated from levels.fyi and Blind"
    )
)
```

This example showcases several structural features that make Great Tables particularly powerful. Row groups automatically organize entries by role, creating clear visual separation. Spanners group related columns under descriptive headers, making the compensation structure immediately clear. The title and subtitle provide essential context at a glance. Source notes add credibility and explain abbreviations like "YoE." And currency formatting ensures proper symbols and separators throughout.

Markdown tables have none of these features. You'd need to manually add separators, can't have multiple header rows, and have no built-in grouping mechanism.

### 4. Extensibility: Start Simple, Add Sophistication

Here's a powerful advantage: once you have Great Tables code from an LLM, you can easily extend it.

Initial basic prompt:

```
Create a simple Great Tables table from sales_data showing revenue formatted
as a number.
```

Initial LLM-generated code (basic):

```python
GT(sales_data).fmt_number(columns="revenue", decimals=0)
```

A follow-up prompt could be this:

```
Now add a title 'Q4 Sales Report', format revenue with thousands separators,
add a growth column formatted as percentage, color-code the growth column
from red to green, make the revenue column bold, and add a source note with
today's date.
```

The LLM extends the original code to this:

```python
(
    GT(sales_data)
    .tab_header(title="Q4 Sales Report")
    .fmt_number(columns="revenue", decimals=0, use_seps=True)
    .fmt_percent(columns="growth", decimals=1)
    .data_color(columns="growth", palette=["red", "yellow", "green"])
    .tab_style(
        style=style.text(weight="bold"),
        locations=loc.body(columns="revenue")
    )
    .tab_source_note("Internal data as of Nov 2025")
)
```

This demonstrates the real power of Great Tables' compositional design. Extensions feel natural because each method call adds just one new feature. You're building up from a solid foundation rather than starting from scratch with every iteration. Typically, the LLM gives you a lot of what you need initially, and you can refine through simple additions until you're satisfied with the output table.

With Markdown you can't extend unless you want to type. You either regenerate the entire table or manually edit text (error-prone and not reproducible).

### 5. Data Updates: Reproducibility is Everything

Perhaps the most underrated advantage: Great Tables code is a *recipe* (not merely a *result*).

Here's a scenario that comes up constantly in real work. You asked an LLM to create a table from Q3 sales data, and it gave you working code. Now it's Q4 and you have new data in the same format. What do you do?

With Great Tables, you simply point the existing code at the new data:

```python
# Original Q3 code that the LLM generated
# q3_data = pl.read_csv("data/q3_sales.csv")
# GT(q3_data).fmt_number(columns="revenue", decimals=0)

# Q4: Just swap the data source
updated_data = pl.read_csv("data/q4_sales.csv")
GT(updated_data).fmt_number(columns="revenue", decimals=0)
```

The code is a reusable recipe. Change the data, get a consistent table (no regeneration needed).

With Markdown tables, you're essentially starting over:

- asking the LLM to regenerate the entire table
- hoping it formats the table consistently
- manually verifying every number
- taking a risk on inconsistent styling between versions

Having code instead of static output fundamentally changes how you work:

- your table definition becomes reusable infrastructure
- weekly/monthly reports are trivial (just swap the data!)
- consistency across time periods is guaranteed
- you can version control the recipe, not the output

## The LLM Sweet Spot: Analysis -> Code -> Insight

When you combine LLMs with Great Tables, you unlock a powerful iterative workflow that's greater than the sum of its parts. Instead of just asking for analysis or just asking for a table, you can have a conversation that progressively refines both the data understanding and its presentation. Here's what this workflow typically looks like in practice:

Here's how Great Tables fits into this workflow:

1. **You**: "Analyze these sales trends and create a table showing regions, current quarter sales, and trend sparklines"

2. **LLM**: generates trend values and Great Tables code with `.fmt_nanoplot()`

3. **You**: see visual trends immediately, spot the declining region, ask follow-up questions

4. **LLM**: updates the code to add conditional formatting on the declining region

5. **Result**: a publication-ready table that's also a reproducible recipe

This workflow is impossible with Markdown tables. You'd be stuck asking the LLM to describe trends in text or regenerating static tables repeatedly.

## Human Visual Processing > Text Processing

Humans are visual creatures. We process information much faster when it's presented visually rather than via text. Consider this example:

**Text description** 

`"The upload endpoint has an average latency of 1,243ms with a P95 of 2,456ms and an error rate of 5.1%, which is significantly higher than other endpoints..."`

**Markdown table**

Better, but you still need to scan and compare numbers mentally.

**Great Tables with color gradients**

Instant visual hit as red cells will jump out immediately! You know there's a problem before you even read the numbers.

LLMs are great at generating structured text, but they can't see the output the way humans do. Great Tables bridges this gap by transforming LLM-generated structured data into visually optimized presentations.

## Why Tables Can Be Better Than Plots for LLM-Generated Analysis

There's another good reason to favor tables when working with LLMs. LLMs can sometimes struggle to accurately interpret plots. Recent research from Posit's [bluffbench evaluation](https://posit.co/blog/introducing-bluffbench/) reveals a surprising limitation. When shown plots of data, LLMs often see what they *expect* to see rather than what's *actually* displayed.

In the bluffbench tests, LLMs were presented with plots from familiar datasets like `mtcars`. However, there's a twist: the data was secretly transformed to flip the expected relationships. For example, they modified the horsepower variable so that higher values actually represented lower horsepower. When asked to interpret a scatter plot showing the relationship between horsepower and fuel efficiency, the LLMs consistently reported seeing the expected negative correlation (even though the plot clearly showed the opposite pattern).

Here's a brief summary of the results:

- with well-known datasets (like `mtcars`), LLMs correctly identified contradictory patterns only about 20-40% of the time
- even with novel datasets containing unexpected discontinuities, accuracy was only around 50-60%
- models tended to describe what they expected based on variable names rather than what was actually plotted

This has obviously consequences for data analysis. When you ask an LLM to analyze data and create visualizations, you can't fully trust its interpretation of the visual output. The LLM might generate a plot, but then describe patterns that align with its expectations rather than what's right there in the image.

Interpreting data in tables can sidestep these issues because tables are text/code. LLMs can directly read table values as structured text, not interpret them from pixels. Because there's no visual ambiguity, there's no gap between what's displayed and what the LLM perceives. There's a precision to tables since LLMs can accurately reference specific numbers, compare values, and identify anomalies. This is great for reproducible analysis given that the table structure itself conveys the data unambiguously.

When you generate a Great Tables table with an LLM, you'll probably get a reliable interpretation of the data within it. The LLM can accurately read numeric values, compare them across rows, identify outliers, and spot trends. And this is without the risk of misinterpreting visual encodings or seeing patterns that align with expectations rather than reality. Where image-based plots can mislead an LLM's pattern recognition, structured table data provides direct, unambiguous access to the actual values.

This doesn't mean plots are bad. Visualizations remain essential for human consumption and pattern recognition. But when working in an LLM-assisted workflow where the AI needs to both generate and interpret data presentations, tables provide a more reliable foundation.

## Practical Tips for Using Great Tables with LLMs

Our testing of three frontier models revealed clear patterns about what works and what doesn't. Here are strategies based on actual experiments with hundreds of prompts:

**1. Provide API documentation when possible**

The single biggest factor in success rates: giving the LLM access to API documentation. Without docs, we saw a 17% success rate. With docs, 73% on the first three examples. This isn't about the LLM "learning" the API, it's about giving it the right context to avoid hallucinating methods.

If you're using GitHub Copilot, Claude, or ChatGPT, share relevant API documentation or examples before asking for code. Even a simple "Use the Great Tables documentation at https://posit-dev.github.io/great-tables/" can help.

**2. Start simple, then layer complexity**

Our testing showed that simple formatting requests (Example 1: basic number formatting + conditional styling) had a 100% success rate even without docs. Complex requests (Example 3: row grouping + multiple spanners + color gradients) failed completely without context.

Start with basic structure and formatting, then iteratively add features:

```
First: "Create a table with sales data, format revenue with commas"
Then: "Add a color gradient to the growth column"
Finally: "Add row groups by region and a title"
```

**3. Be explicit about column names and data structure**

One of the most common failures: LLMs hallucinating column names. If your data has `p99_ms` but the LLM assumes it's `p99`, the code crashes.

Share your data structure upfront:

```
"The CSV has columns: endpoint, method, avg_ms, p95_ms, p99_ms, requests, errors.
Create a Great Tables table that formats the millisecond columns..."
```

**4. Specify formatting types precisely**

Don't say "format nicely" or "make it look good". Say exactly what you want:

- ✓ "Format salary columns as USD currency with no decimals"
- ✓ "Add thousands separators to the requests column"
- ✓ "Use a blue color gradient on the stars column from 0 to 40000"

Our testing showed that vague requests led to hallucinated methods like `.fmt_nice()` or `.auto_format()`.

**5. Use Great Tables' compositional syntax to your advantage**

Great Tables methods chain naturally, which maps well to how LLMs generate code. When asking for multiple features, list them in the order they'd appear in the code:

```
"Load tech_salaries.csv, then create a Great Tables table that:
- Groups rows by role
- Formats compensation columns as currency
- Adds a color gradient to total_comp
- Includes column spanners for compensation columns"
```

This structure helps LLMs generate correct method chaining.

**6. Leverage nanoplots for trend data**

In our testing, LLMs handled `.fmt_nanoplot()` surprisingly well when given clear instructions. Trend data stored as comma-separated values (like `"195,210,228,245"`) translates naturally to sparklines with this single method call.

## Testing Your Prompts Systematically with chatlas

If you're building a workflow that relies on LLM-generated Great Tables code, you'll want to test your prompts systematically rather than manually checking each output. The [chatlas](https://posit-dev.github.io/chatlas/) Python package provides a powerful way to create automated evaluations.

### A Complete Working Example

We've created a [full evaluation notebook](https://github.com/rich-iannone/gt-and-llms/blob/main/chatlas-eval-analysis.qmd) that demonstrates this workflow using [Inspect](https://inspect.aisi.org.uk), and it shows you how to:

1. define test cases with specific prompts and target descriptions
2. use Inspect to create structured evaluation tasks with samples, solvers, and scorers
3. implement a custom scorer that judges code quality as **poor**, **fair**, or **great** based on feature completeness (not just execution success)
4. run systematic evaluations comparing results with and without API documentation
5. visualize results using Great Tables itself

The notebook uses chatlas within the Inspect framework to automatically generate code from prompts and evaluates it with an LLM judge. This approach goes beyond simple pass/fail testing to provide nuanced quality ratings that reflect whether the generated code contains the requested features and uses appropriate methods.

Check out the full notebook to see the complete implementation, including how to handle code extraction, add missing imports automatically, and create compelling visualizations of evaluation results.

With systematic evals, you can:

- validate that documentation helps through quantifying exactly how much API context improves success rates
- test prompt strategies and experiment with different system prompts and measure results
- catch regressions by re-running tests when LLM providers update models
- build confidence knowing your prompts work reliably before deploying workflows

If you're building a data app that generates tables or creating tools that help users create Great Tables code, this eval framework lets you validate that your prompts actually work.

## In Conclusion

When you're analyzing data in a notebook with LLM assistance, you need tables that are:

- visual: easy for people to process information at a glance
- reproducible: run the same code with new data
- extensible: start simple, add sophistication as needed
- structured: support titles, groups, spanners, footnotes
- intelligent: conditional formatting, color gradients, sparklines

Markdown tables are fine for simple, static displays. But when you're doing real analysis work, Great Tables gives you high-quality output with programmatic control. Just as you'd use plotnine or seaborn for visualizations in your notebook, we believe that Great Tables is the right choice for tables. And when you combine it with LLM assistance, you get the best of both worlds: conversational specification with code-level control.

The future of data analysis involves people and LLMs working together. Great Tables is built for this kind of collaboration and it gives you the structure and tools you need while remaining accessible enough for LLMs to generate valid code.

Try it out in your next notebook! Ask your favorite LLM to create a Great Tables table from your data. You might be surprised how well it works. And you'll likely be delighted with how much more effective your tables become.
