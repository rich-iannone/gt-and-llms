---
title: "Why Use Great Tables with LLMs?"
author: Rich Iannone
date: 2025-11-13
jupyter: python3
format:
    html:
        embed-resources: true
html-table-processing: none
---

## The Problem: LLMs Love Tables, But Notebooks Need Better

If you've used Claude, ChatGPT, or other LLM interfaces lately, you've noticed something: they love presenting information in tables. And for good reason. Tables are incredibly effective at organizing structured information.

But here's the disconnect: when you're analyzing data in a Python notebook, you need tables that live in your code, not just in a chat interface. You want the same control over tables that you have with plotnine or seaborn for visualizations. You want reproducible, extensible, and beautiful tables that work with your data pipelines.

This is where Great Tables shines. Great Tables is a Python library designed for quickly generating sophisticated tables in your notebook, whether you're writing the code yourself or working with an LLM assistant. More importantly, it gives you something Markdown tables can't: a programmatic recipe that adapts to your data and extends with your needs.

Let's explore why Great Tables is the ideal choice when working with LLMs to analyze and present data.

## The Progression: Text -> Markdown -> Great Tables

To understand the value of Great Tables, let's see how the same information looks when presented in three different ways. We'll use a realistic scenario: analyzing GitHub repository metrics.

### Stage 1: Paragraph Text

Here's how an LLM might initially describe repository data:

```
The great-tables repository has 1,834 stars and 67 forks, with 23 open issues and 234 closed issues. It has merged 456 pull requests and the last commit was 2 days ago. The polars repository is much larger with 23,456 stars and 1,234 forks, currently has 345 open issues with 5,678 closed, has merged 8,934 pull requests, and was updated 1 day ago. The pandas repository is the largest with 38,934 stars...
```

The problem with this chunk of text is that it is exhausting to read. Finding patterns or comparing values requires some mental gymnastics here. You can't quickly spot which repos are most active or which need attention.

### Stage 2: Markdown Table

The natural next step is to structure this information in a table format. This is exactly what LLMs often do when you ask them to present data more clearly. Let's improve this with a Markdown table:

```markdown
| Repository | Language | Stars | Forks | Open Issues | Closed Issues | PRs Merged | Days Since Commit |
|------------|----------|-------|-------|-------------|---------------|------------|-------------------|
| great-tables | Python | 1834 | 67 | 23 | 234 | 456 | 2 |
| polars | Rust | 23456 | 1234 | 345 | 5678 | 8934 | 1 |
| pandas | Python | 38934 | 15678 | 2345 | 23456 | 12345 | 3 |
| duckdb | C++ | 15234 | 890 | 123 | 3456 | 5678 | 1 |
| quarto-cli | TypeScript | 8234 | 456 | 89 | 1234 | 2345 | 5 |
| plotly.py | Python | 13456 | 2345 | 234 | 4567 | 6789 | 4 |
```

Much better! The table structure makes comparisons easier. But we're still facing limitations:

- numbers aren't formatted for readability (1834 vs 1,834)
- no visual cues for what's important
- can't highlight concerning patterns (old commits, high open issues)
- static, so if data changes, you need to regenerate the entire table
- no room for additional context (titles, footnotes, sources)
